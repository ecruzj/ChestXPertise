{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebcc9fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "# Bar Progress and import images\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from PIL import Image\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8220226c",
   "metadata": {},
   "source": [
    "### Load the Images and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6869f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to your data directories\n",
    "path = 'multiclass_x-ray-images/'\n",
    "out= 'models/'\n",
    "train_dataset_path = os.path.join(path, 'set_train')\n",
    "validation_dataset_path = os.path.join(path, 'set_test')\n",
    "test_dataset_path = os.path.join(path, 'set_pred')  # Assuming you have a 'set_pred' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d576b",
   "metadata": {},
   "source": [
    "### Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b900a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, folder_dir, image_size):\n",
    "        self.image_paths = []  # List of image paths\n",
    "        self.image_labels = []  # List of image labels\n",
    "        self.classes = []  # List of class labels\n",
    "\n",
    "        # Define list of image transformations\n",
    "        image_transformation = [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "\n",
    "        self.image_transformation = transforms.Compose(image_transformation)\n",
    "\n",
    "        # Get all image paths, labels, and classes\n",
    "        categories = os.listdir(folder_dir)\n",
    "        for category_index, category in enumerate(categories):\n",
    "            category_path = os.path.join(folder_dir, category)\n",
    "            if os.path.isdir(category_path):\n",
    "                images = os.listdir(category_path)\n",
    "                for image in images:\n",
    "                    image_path = os.path.join(category_path, image)\n",
    "                    self.image_paths.append(image_path)\n",
    "                    label = [0] * len(categories)\n",
    "                    label[category_index] = 1\n",
    "                    self.image_labels.append(label)\n",
    "                self.classes.append(category)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Read image\n",
    "        image_path = self.image_paths[index]\n",
    "        image_data = Image.open(image_path).convert(\"RGB\")  # Convert image to RGB channels\n",
    "\n",
    "        # Resize and convert image to torch tensor\n",
    "        image_data = self.image_transformation(image_data)\n",
    "\n",
    "        return image_data, torch.FloatTensor(self.image_labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5431d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_mappings(data_loader, loader_type):\n",
    "    # Extract class labels from the dataset\n",
    "    class_labels = data_loader.dataset.classes\n",
    "\n",
    "    # Create a dictionary to map class indices to class labels\n",
    "    labels = {index: label for index, label in enumerate(class_labels)}\n",
    "\n",
    "    # Print the label mappings with loader type\n",
    "    print(f\"Label Mappings for classes present in the {loader_type} dataset\\n\")\n",
    "    for key, value in labels.items():\n",
    "        print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326cd4e",
   "metadata": {},
   "source": [
    "## Define the Models\n",
    "<li> 1. CustomNet\n",
    "<li> 2. DenseNet121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d277eb",
   "metadata": {},
   "source": [
    "### CustomNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed6edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=8, is_trained=False):\n",
    "        super().__init__()\n",
    "        self.ConvLayer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 3),\n",
    "            nn.Conv2d(8, 16, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ConvLayer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5),\n",
    "            nn.Conv2d(32, 32, 3),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ConvLayer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.Conv2d(64, 64, 5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ConvLayer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 5),\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Lin1 = nn.Sequential(nn.Linear(512, num_classes), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ConvLayer1(x)\n",
    "        x = self.ConvLayer2(x)\n",
    "        x = self.ConvLayer3(x)\n",
    "        x = self.ConvLayer4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.Lin1(x)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed04ae",
   "metadata": {},
   "source": [
    "## Pre Trained Models\n",
    "### DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac24996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        Init model architecture\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes: int\n",
    "            number of classes\n",
    "        is_trained: bool\n",
    "            whether using pretrained model from ImageNet or not\n",
    "\"\"\"\n",
    "####################################################################################\n",
    "###   DenseNet121\n",
    "####################################################################################\n",
    "#\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, num_classes=8, is_trained=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.net = torchvision.models.densenet121(pretrained=is_trained)\n",
    "        # Get the input dimension of last layer\n",
    "        kernel_count = self.net.classifier.in_features\n",
    "        self.net.classifier = nn.Sequential(nn.Linear(kernel_count, num_classes), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward the netword with the inputs\n",
    "        \"\"\"\n",
    "        return self.net(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157cff3",
   "metadata": {},
   "source": [
    "### Setting Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac0873f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224                              # Image size (224x224)\n",
    "BATCH_SIZE = 96\n",
    "LEARNING_RATE = 0.001\n",
    "LEARNING_RATE_SCHEDULE_FACTOR = 0.1           # Parameter used for reducing learning rate\n",
    "LEARNING_RATE_SCHEDULE_PATIENCE = 5           # Parameter used for reducing learning rate\n",
    "MAX_EPOCHS = 50   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c7fac",
   "metadata": {},
   "source": [
    "### Training, Validation & Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094c083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ChestXrayDataset(train_dataset_path, IMAGE_SIZE)\n",
    "validation_dataset = ChestXrayDataset(validation_dataset_path, IMAGE_SIZE)\n",
    "test_dataset = ChestXrayDataset(test_dataset_path, IMAGE_SIZE)  # For prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb827b8f",
   "metadata": {},
   "source": [
    "### Training, Validation & Test Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6167bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)  # No need to shuffle for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698cbf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mappings for classes present in the training dataset\n",
      "\n",
      "0 : Atelectasis\n",
      "1 : Effusion\n",
      "2 : Infiltration\n",
      "3 : Mass\n",
      "4 : No Finding\n",
      "5 : Nodule\n",
      "6 : Pneumonia\n",
      "7 : Pneumothorax\n"
     ]
    }
   ],
   "source": [
    "# show the categories for train_loader\n",
    "print_label_mappings(train_dataloader, \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0965bde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mappings for classes present in the validation dataset\n",
      "\n",
      "0 : Atelectasis\n",
      "1 : Effusion\n",
      "2 : Infiltration\n",
      "3 : Mass\n",
      "4 : No Finding\n",
      "5 : Nodule\n",
      "6 : Pneumonia\n",
      "7 : Pneumothorax\n"
     ]
    }
   ],
   "source": [
    "# show the categories for validation_loader\n",
    "print_label_mappings(val_dataloader, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb0c655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Atelectasis',\n",
       " 'Effusion',\n",
       " 'Infiltration',\n",
       " 'Mass',\n",
       " 'No Finding',\n",
       " 'Nodule',\n",
       " 'Pneumonia',\n",
       " 'Pneumothorax']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = train_dataloader.dataset.classes\n",
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca3cb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_dataset\n",
    "del validation_dataset\n",
    "del test_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7ab2b",
   "metadata": {},
   "source": [
    "### Set Divice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf02560e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ea044",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc00ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_auroc(y_gt, y_pred):\n",
    "    \"\"\" Calculate AUROC for each class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_gt: torch.Tensor\n",
    "        groundtruth\n",
    "    y_pred: torch.Tensor\n",
    "        prediction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        F1 of each class\n",
    "    \"\"\"\n",
    "    auroc = []\n",
    "    gt_np = y_gt.to(\"cpu\").numpy()\n",
    "    pred_np = y_pred.to(\"cpu\").numpy()\n",
    "    assert gt_np.shape == pred_np.shape, \"y_gt and y_pred should have the same size\"\n",
    "    for i in range(gt_np.shape[1]):\n",
    "        try:\n",
    "            auroc.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return auroc\n",
    "\n",
    "def multi_label_accuracy(y_gt, y_pred):\n",
    "    \"\"\" Calculate AUROC for each class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_gt: torch.Tensor\n",
    "        groundtruth\n",
    "    y_pred: torch.Tensor\n",
    "        prediction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        F1 of each class\n",
    "    \"\"\"\n",
    "    acc = []\n",
    "    gt_np = y_gt.to(\"cpu\").numpy()\n",
    "    pred_np = y_pred.to(\"cpu\").numpy()\n",
    "    assert gt_np.shape == pred_np.shape, \"y_gt and y_pred should have the same size\"\n",
    "    for i in range(gt_np.shape[1]):\n",
    "        acc.append(accuracy_score(gt_np[:, i], np.where(pred_np[:, i]>=0.5,1,0)))\n",
    "    return acc\n",
    "\n",
    "def multi_label_f1(y_gt, y_pred):\n",
    "    \"\"\" Calculate f1 for each class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_gt: torch.Tensor\n",
    "        groundtruth\n",
    "    y_pred: torch.Tensor\n",
    "        prediction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        F1 of each class\n",
    "    \"\"\"\n",
    "    f1_out = []\n",
    "    gt_np = y_gt.to(\"cpu\").numpy()\n",
    "    pred_np = y_pred.to(\"cpu\").numpy()\n",
    "    assert gt_np.shape == pred_np.shape, \"y_gt and y_pred should have the same size\"\n",
    "    for i in range(gt_np.shape[1]):\n",
    "        f1_out.append(f1_score(gt_np[:, i], np.where(pred_np[:, i]>=0.5,1,0)))\n",
    "    return f1_out\n",
    "\n",
    "\n",
    "def multi_label_precision_recall(y_gt, y_pred):\n",
    "    \"\"\" Calculate precision for each class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_gt: torch.Tensor\n",
    "        groundtruth\n",
    "    y_pred: torch.Tensor\n",
    "        prediction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        precision of each class\n",
    "    \"\"\"\n",
    "    precision_out = []\n",
    "    recall_out = []\n",
    "    gt_np = y_gt.to(\"cpu\").numpy()\n",
    "    pred_np = y_pred.to(\"cpu\").numpy()\n",
    "    assert gt_np.shape == pred_np.shape, \"y_gt and y_pred should have the same size\"\n",
    "    for i in range(gt_np.shape[1]):\n",
    "        p = precision_recall_fscore_support(gt_np[:, i], np.where(pred_np[:, i]>=0.5,1,0),average='binary')\n",
    "        precision_out.append(p[0])\n",
    "        recall_out.append(p[1])\n",
    "    return precision_out,recall_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5712bf2",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba0c76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_training(epoch, model, train_dataloader, device, loss_criteria, optimizer, mb):\n",
    "    \"\"\"\n",
    "    Epoch training\n",
    "\n",
    "    Paramteters\n",
    "    -----------\n",
    "    epoch: int\n",
    "      epoch number\n",
    "    model: torch Module\n",
    "      model to train\n",
    "    train_dataloader: Dataset\n",
    "      data loader for training\n",
    "    device: str\n",
    "      \"cpu\" or \"cuda\"\n",
    "    loss_criteria: loss function\n",
    "      loss function used for training\n",
    "    optimizer: torch optimizer\n",
    "      optimizer used for training\n",
    "    mb: master bar of fastprogress\n",
    "      progress to log\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "      training loss\n",
    "    \"\"\"\n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "    training_loss = 0 # Storing sum of training losses\n",
    "\n",
    "    # For each batch\n",
    "    for batch, (images, labels) in enumerate(progress_bar(train_dataloader, parent=mb)):\n",
    "\n",
    "        # Move X, Y  to device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear previous gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed forward the model\n",
    "        pred = model(images)\n",
    "        #pred = torch.LongTensor(pred)\n",
    "        loss = loss_criteria(pred, labels)\n",
    "        #print(\"loss is \",loss)\n",
    "\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update training loss after each batch\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        #mb.child.comment = f'Training loss {training_loss/(batch+1)}'\n",
    "\n",
    "    del images, labels, loss\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    # return training loss\n",
    "    return training_loss/len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da5597",
   "metadata": {},
   "source": [
    "### Evaluating Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a8a3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating(epoch, model, val_loader, device, loss_criteria, mb):\n",
    "    \"\"\"\n",
    "    Validate model on validation dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch: int\n",
    "        epoch number\n",
    "    model: torch Module\n",
    "        model used for validation\n",
    "    val_loader: Dataset\n",
    "        data loader of validation set\n",
    "    device: str\n",
    "        \"cuda\" or \"cpu\"\n",
    "    loss_criteria: loss function\n",
    "      loss function used for training\n",
    "    mb: master bar of fastprogress\n",
    "      progress to log\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        loss on validation set\n",
    "    float\n",
    "        metric score on validation set\n",
    "    \"\"\"\n",
    "\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0                                   # Total loss of model on validation set\n",
    "    out_pred = torch.FloatTensor().to(device)      # Tensor stores prediction values\n",
    "    out_gt = torch.FloatTensor().to(device)        # Tensor stores groundtruth values\n",
    "\n",
    "    with torch.no_grad(): # Turn off gradient\n",
    "        # For each batch\n",
    "        for step, (images, labels) in enumerate(progress_bar(val_loader, parent=mb)):\n",
    "            # Move images, labels to device (GPU)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Update groundtruth values\n",
    "            out_gt = torch.cat((out_gt,  labels), 0)\n",
    "\n",
    "            # Feed forward the model\n",
    "            ps = model(images)\n",
    "            loss = loss_criteria(ps, labels)\n",
    "\n",
    "            # Update prediction values\n",
    "            out_pred = torch.cat((out_pred, ps), 0)\n",
    "\n",
    "            # Update validation loss after each batch\n",
    "            val_loss += loss\n",
    "\n",
    "    # Clear memory\n",
    "    del images, labels, loss\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    # return validation loss, and metric score\n",
    "    val_loss_mean = val_loss/len(val_loader)\n",
    "    auroc_mean = np.nanmean(np.array(multi_label_auroc(out_gt, out_pred)))\n",
    "    acc_mean = np.nanmean(np.array(multi_label_accuracy(out_gt, out_pred)))\n",
    "    f1_mean = np.nanmean(np.array(multi_label_f1(out_gt, out_pred)))\n",
    "\n",
    "    return val_loss_mean,auroc_mean,acc_mean,f1_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df634",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a90ea7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt(modeltxt,model):\n",
    "    \n",
    "    if modeltxt == \"CustomNet\":\n",
    "        return optim.Adam(model.parameters())\n",
    "    \n",
    "    # DenseNet121\n",
    "    return optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84748618",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7092b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_losses = []\n",
    "# validation_losses = []\n",
    "# validation_score = []\n",
    "# validation_acc = []\n",
    "# validation_f1 = []\n",
    "\n",
    "# def trainModel(modelname,loss_criteria,modeltxt):\n",
    "#     model = modelname(num_classes=len(LABELS),is_trained=True).to(device)\n",
    "\n",
    "#     optimizer = get_opt(modeltxt,model)\n",
    "#     # Learning rate will be reduced automatically during training\n",
    "#     lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = LEARNING_RATE_SCHEDULE_FACTOR,\n",
    "#                                                         patience = LEARNING_RATE_SCHEDULE_PATIENCE, mode = 'max', verbose=True)\n",
    "#     best_score = 0\n",
    "#     best_score_acc = 0\n",
    "#     best_score_f1 = 0\n",
    "\n",
    "#     model_path = out+modeltxt+\".pth\"\n",
    "#     out_path = out+modeltxt+\"_running.csv\"\n",
    "# #     training_losses = []\n",
    "# #     validation_losses = []\n",
    "# #     validation_score = []\n",
    "# #     validation_acc = []\n",
    "# #     validation_f1 = []\n",
    "\n",
    "\n",
    "#     # Config progress bar\n",
    "#     mb = master_bar(range(MAX_EPOCHS))\n",
    "#     mb.names = ['Train loss', 'Val loss', 'AUROC', 'Accuracy', 'f1 score']\n",
    "#     x = []\n",
    "\n",
    "#     nonimproved_epoch = 0\n",
    "#     start_time = time.time()\n",
    "#     cnt = 1\n",
    "\n",
    "#     # Training each epoch\n",
    "#     for epoch in mb:\n",
    "#         #break\n",
    "#         mb.main_bar.comment = f'Best AUROC score: {best_score}'\n",
    "#         x.append(epoch)\n",
    "\n",
    "#         # Training\n",
    "#         train_loss = epoch_training(epoch, model, train_dataloader, device, loss_criteria, optimizer, mb)\n",
    "#         mb.write('Finish training epoch {} with loss {:.4f}'.format(epoch, train_loss))\n",
    "#         training_losses.append(train_loss)\n",
    "\n",
    "#         # Evaluating\n",
    "#         val_loss, new_score, new_score_acc, new_score_f1 = evaluating(epoch, model, val_dataloader, device, loss_criteria, mb)\n",
    "\n",
    "#         validation_losses.append(val_loss)\n",
    "#         validation_score.append(new_score)\n",
    "#         validation_acc.append(new_score_acc)\n",
    "#         validation_f1.append(new_score_f1)\n",
    "\n",
    "#         gc.collect()\n",
    "#         # Update learning rate\n",
    "#         lr_scheduler.step(new_score)\n",
    "\n",
    "#         # Update training chart\n",
    "#         # mb.update_graph([[x, training_losses], [x, validation_losses], [x, validation_score] , [x, validation_acc] ,\n",
    "#         #                  [x, validation_f1]],\n",
    "#         #                 [0,epoch+1+round(epoch*0.3)], [0,1])\n",
    "\n",
    "#         diff = np.round(time.time() - start_time)\n",
    "#         pd.DataFrame([[epoch,modeltxt,best_score,new_score,diff]]).to_csv(out_path,index=False,mode='a',header=False)\n",
    "#         # Save model\n",
    "#         t2 = 4\n",
    "#         if modeltxt == 'DenseNet121':\n",
    "#             t2 = 6\n",
    "#         if best_score < new_score:\n",
    "#             #mb.write(f\"Improve AUROC from {best_score} to {new_score}\")\n",
    "#             best_score = new_score\n",
    "#             best_score_acc = new_score_acc\n",
    "#             best_score_f1 = new_score_f1\n",
    "#             nonimproved_epoch = 0\n",
    "#             best_model = model\n",
    "#             torch.save({\"model\": model.state_dict(),\n",
    "#                         \"optimizer\": optimizer.state_dict(),\n",
    "#                         \"best_score\": best_score,\n",
    "#                         \"epoch\": epoch,\n",
    "#                         \"lr_scheduler\": lr_scheduler.state_dict()}, model_path)\n",
    "#         else:\n",
    "#             nonimproved_epoch += 1\n",
    "#         if nonimproved_epoch > 5:\n",
    "#             break\n",
    "#             print(\"Early stopping\")\n",
    "#         if time.time() - start_time > 3600*t2:\n",
    "#             break\n",
    "#             print(\"Out of time\")\n",
    "            \n",
    "#     return best_score,best_score_acc,best_score_f1,best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3239d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(modelname, loss_criteria, modeltxt):\n",
    "    model = modelname(num_classes=len(LABELS), is_trained=True).to(device)\n",
    "\n",
    "    optimizer = get_opt(modeltxt, model)\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=LEARNING_RATE_SCHEDULE_FACTOR,\n",
    "                                                        patience=LEARNING_RATE_SCHEDULE_PATIENCE, mode='max', verbose=True)\n",
    "    best_score = 0\n",
    "    best_score_acc = 0\n",
    "    best_score_f1 = 0\n",
    "\n",
    "    model_path = out + modeltxt + \".pth\"\n",
    "    out_path = out + modeltxt + \"_running.csv\"\n",
    "    \n",
    "    # Use a dictionary to store values for each model\n",
    "    model_results = {\n",
    "        'CustomNet': {'training_losses': [], 'validation_losses': [], 'validation_score': [], 'validation_acc': [], 'validation_f1': []},\n",
    "        'DenseNet121': {'training_losses': [], 'validation_losses': [], 'validation_score': [], 'validation_acc': [], 'validation_f1': []}\n",
    "    }\n",
    "\n",
    "    mb = master_bar(range(MAX_EPOCHS))\n",
    "    mb.names = ['Train loss', 'Val loss', 'AUROC', 'Accuracy', 'f1 score']\n",
    "    x = []\n",
    "\n",
    "    nonimproved_epoch = 0\n",
    "    start_time = time.time()\n",
    "    cnt = 1\n",
    "\n",
    "    for epoch in mb:\n",
    "        mb.main_bar.comment = f'Best AUROC score: {best_score}'\n",
    "        x.append(epoch)\n",
    "\n",
    "        train_loss = epoch_training(epoch, model, train_dataloader, device, loss_criteria, optimizer, mb)\n",
    "        mb.write('Finish training epoch {} with loss {:.4f}'.format(epoch, train_loss))\n",
    "        model_results[modeltxt]['training_losses'].append(train_loss)\n",
    "\n",
    "        val_loss, new_score, new_score_acc, new_score_f1 = evaluating(epoch, model, val_dataloader, device, loss_criteria, mb)\n",
    "\n",
    "        model_results[modeltxt]['validation_losses'].append(val_loss)\n",
    "        model_results[modeltxt]['validation_score'].append(new_score)\n",
    "        model_results[modeltxt]['validation_acc'].append(new_score_acc)\n",
    "        model_results[modeltxt]['validation_f1'].append(new_score_f1)\n",
    "\n",
    "        gc.collect()\n",
    "        lr_scheduler.step(new_score)\n",
    "\n",
    "        diff = np.round(time.time() - start_time)\n",
    "        pd.DataFrame([[epoch, modeltxt, best_score, new_score, diff]]).to_csv(out_path, index=False, mode='a', header=False)\n",
    "\n",
    "        t2 = 4\n",
    "        if modeltxt == 'DenseNet121':\n",
    "            t2 = 6\n",
    "        if best_score < new_score:\n",
    "            best_score = new_score\n",
    "            best_score_acc = new_score_acc\n",
    "            best_score_f1 = new_score_f1\n",
    "            nonimproved_epoch = 0\n",
    "            best_model = model\n",
    "            torch.save({\"model\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"best_score\": best_score,\n",
    "                        \"epoch\": epoch,\n",
    "                        \"lr_scheduler\": lr_scheduler.state_dict()}, model_path)\n",
    "        else:\n",
    "            nonimproved_epoch += 1\n",
    "        if nonimproved_epoch > 5:\n",
    "            break\n",
    "            print(\"Early stopping\")\n",
    "        if time.time() - start_time > 3600 * t2:\n",
    "            break\n",
    "            print(\"Out of time\")\n",
    "\n",
    "    return best_score, best_score_acc, best_score_f1, best_model, model_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe7b18",
   "metadata": {},
   "source": [
    "### Set Models to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b8fe32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [CustomNet, DenseNet121]\n",
    "mName_list = ['CustomNet', 'DenseNet121']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12f3b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df_train = []\n",
    "# model_results_dict = {}\n",
    "\n",
    "# for m in model_list:\n",
    "#     mName = m().__class__.__name__\n",
    "#     print(\"Processing Model \",mName)\n",
    "#     globals()[f\"best_score_{mName}\"],globals()[f\"best_score_acc_{mName}\"],globals()[f\"best_score_f1_{mName}\"],globals()[f\"best_model_{mName}\"], model_results = trainModel(modelname=m,loss_criteria=nn.BCELoss(),modeltxt=mName)\n",
    "#     #\n",
    "#     eval_df_train.append([mName,globals()[f\"best_score_{mName}\"],globals()[f\"best_score_acc_{mName}\"],globals()[f\"best_score_f1_{mName}\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002be01",
   "metadata": {},
   "source": [
    "### Train Models in a Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79115732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Model  CustomNet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.00% [1/50 32:49&lt;26:48:07 Best AUROC score: 0]\n",
       "    </div>\n",
       "    \n",
       "Finish training epoch 0 with loss 0.3877<p>Finish training epoch 1 with loss 0.3734<p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='4' class='' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.26% [4/39 00:22&lt;03:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df_train = []\n",
    "model_results_dict = {}\n",
    "\n",
    "for m in model_list:\n",
    "    mName = m().__class__.__name__\n",
    "    print(\"Processing Model \", mName)\n",
    "    globals()[f\"best_score_{mName}\"],globals()[f\"best_score_acc_{mName}\"],globals()[f\"best_score_f1_{mName}\"],globals()[f\"best_model_{mName}\"], model_results = trainModel(modelname=m, loss_criteria=nn.BCELoss(), modeltxt=mName)\n",
    "\n",
    "    # Store results in model_results_dict\n",
    "    model_results_dict[mName] = model_results\n",
    "\n",
    "    eval_df_train.append([mName,globals()[f\"best_score_{mName}\"],globals()[f\"best_score_acc_{mName}\"],globals()[f\"best_score_f1_{mName}\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af08dd1",
   "metadata": {},
   "source": [
    "### Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_train=pd.DataFrame(eval_df_train)\n",
    "eval_df_train.columns = ['Model Name','AUROC', 'Accuracy', 'f1 Score']\n",
    "eval_df_train.to_csv(out+\"eval_df_train.csv\",index=False)\n",
    "eval_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access results for each model from model_results_dict\n",
    "for mName, model_result in model_results_dict.items():\n",
    "    print(f\"Results for {mName}:\")\n",
    "    print(f\"Training Losses: {model_result[mName]['training_losses']}\")\n",
    "    print(f\"Validation Losses: {model_result[mName]['validation_losses']}\")\n",
    "    print(f\"Validation Scores: {model_result[mName]['validation_score']}\")\n",
    "    print(f\"Validation Accuracies: {model_result[mName]['validation_acc']}\")\n",
    "    print(f\"Validation F1 Scores: {model_result[mName]['validation_f1']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84909b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis for the plots\n",
    "fig, axs = plt.subplots(len(model_list), 2, figsize=(16, 8 * len(model_list)))\n",
    "fig.suptitle('Training and Validation Metrics Over Epochs for Each Model', y=1)\n",
    "\n",
    "for i, m in enumerate(model_list):\n",
    "    mName = m().__class__.__name__\n",
    "    \n",
    "    # Access results for the current model from model_results_dict\n",
    "    model_result = model_results_dict[mName]\n",
    "\n",
    "    # Access the epoch values from the model_results_dict\n",
    "    x = model_result[mName]['training_losses']\n",
    "\n",
    "    # Create a DataFrame for the metrics\n",
    "    df_metrics = pd.DataFrame({\n",
    "        'Epoch': range(len(x)),\n",
    "        'Training Loss': model_result[mName]['training_losses'],\n",
    "        'Validation Loss': model_result[mName]['validation_losses'],\n",
    "        'AUROC': model_result[mName]['validation_score'],\n",
    "        'Accuracy': model_result[mName]['validation_acc'],\n",
    "        'F1 Score': model_result[mName]['validation_f1']\n",
    "    })\n",
    "\n",
    "    # Plot training loss\n",
    "    axs[i, 0].plot(df_metrics['Epoch'], df_metrics['Training Loss'], label='Training Loss')\n",
    "    axs[i, 0].set_title(f'{mName} - Training Loss')\n",
    "    axs[i, 0].set_xlabel('Epoch')\n",
    "    axs[i, 0].set_ylabel('Training Loss')\n",
    "    axs[i, 0].legend()\n",
    "\n",
    "    # Plot validation metrics\n",
    "    axs[i, 1].plot(df_metrics['Epoch'], df_metrics['Validation Loss'], label='Validation Loss')\n",
    "    axs[i, 1].plot(df_metrics['Epoch'], df_metrics['AUROC'], label='AUROC')\n",
    "    axs[i, 1].plot(df_metrics['Epoch'], df_metrics['Accuracy'], label='Accuracy')\n",
    "    axs[i, 1].plot(df_metrics['Epoch'], df_metrics['F1 Score'], label='F1 Score')\n",
    "    axs[i, 1].set_title(f'{mName} - Validation Metrics')\n",
    "    axs[i, 1].set_xlabel('Epoch')\n",
    "    axs[i, 1].set_ylabel('Metric Value')\n",
    "    axs[i, 1].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout for the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac62609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis for the plots\n",
    "fig, axs = plt.subplots(len(model_list), 2, figsize=(16, 8 * len(model_list)))\n",
    "fig.suptitle('Training and Validation Metrics Over Epochs for Each Model', y=1)\n",
    "\n",
    "for i, m in enumerate(model_list):\n",
    "    mName = m().__class__.__name__\n",
    "    \n",
    "    # Access results for the current model from model_results_dict\n",
    "    model_result = model_results_dict[mName]\n",
    "\n",
    "    # Access the epoch values from the model_results_dict\n",
    "    x = model_result[mName]['training_losses']\n",
    "\n",
    "    # Create a DataFrame for the metrics\n",
    "    df_metrics = pd.DataFrame({\n",
    "        'Epoch': range(len(x)),\n",
    "        'Training Loss': model_result[mName]['training_losses'],\n",
    "        'Validation Loss': model_result[mName]['validation_losses'],\n",
    "        'AUROC': model_result[mName]['validation_score'],\n",
    "        'Accuracy': model_result[mName]['validation_acc'],\n",
    "        'F1 Score': model_result[mName]['validation_f1']\n",
    "    })\n",
    "\n",
    "    # Plot training loss with validation loss\n",
    "    axs[i, 0].plot(df_metrics['Epoch'], df_metrics['Training Loss'], label='Training Loss')\n",
    "    axs[i, 0].plot(df_metrics['Epoch'], df_metrics['Validation Loss'], label='Validation Loss')  # Add this line\n",
    "    axs[i, 0].set_title(f'{mName} - Training and Validation Loss')\n",
    "    axs[i, 0].set_xlabel('Epoch')\n",
    "    axs[i, 0].set_ylabel('Loss')\n",
    "    axs[i, 0].legend()\n",
    "\n",
    "    # Plot validation metrics\n",
    "    axs[i, 1].plot(df_metrics['Epoch'], df_metrics['Validation Loss'], label='Validation Loss')\n",
    "    axs[i, 1].plot(df_metrics['Epoch'], df_metrics['AUROC'], label='AUROC')\n",
    "    axs[i, 1].plot(df_metrics['Epoch'], df_metrics['Accuracy'], label='Accuracy')\n",
    "    axs[i, 1].plot(df_metrics['Epoch'], df_metrics['F1 Score'], label='F1 Score')\n",
    "    axs[i, 1].set_title(f'{mName} - Validation Metrics')\n",
    "    axs[i, 1].set_xlabel('Epoch')\n",
    "    axs[i, 1].set_ylabel('Metric Value')\n",
    "    axs[i, 1].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout for the title\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7061f",
   "metadata": {},
   "source": [
    "### Best model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03697573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_parameters_all(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = []\n",
    "i = 0\n",
    "for model in model_list:\n",
    "    modelName = model().__class__.__name__\n",
    "    num_params_all = count_parameters_all(model())\n",
    "    num_params = count_parameters(model())\n",
    "    param_list.append([modelName,num_params_all,num_params])\n",
    "\n",
    "param_list=pd.DataFrame(param_list)\n",
    "param_list.columns = ['Model Name','Total Parameters','Trainable Parameters']\n",
    "param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86903d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e32335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters_pretty(model):\n",
    "    print(model.__class__.__name__)\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    k = f\"Total Trainable Params: {total_params}\"\n",
    "    return k\n",
    "\n",
    "out1=count_parameters_pretty(best_model_CustomNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_dataloader\n",
    "# del val_dataloader\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21537c8a",
   "metadata": {},
   "source": [
    "### Functions for Prediction, Confusion Matrix and Plots on Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestPreds(best_model,test_dataloader,modeltxt):\n",
    "    y_pred_t = torch.FloatTensor().to(device) # Tensor stores prediction values\n",
    "    y_test_t = torch.FloatTensor().to(device) # Tensor stores groundtruth values\n",
    "\n",
    "    y_pred_list = []\n",
    "    y_test_list = []\n",
    "    test_auroc = []\n",
    "    test_acc = []\n",
    "    test_f1 = []\n",
    "    test_precision = []\n",
    "    test_recall = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        for X_batch, labels in test_dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ps = best_model(X_batch)\n",
    "            y_test_t = torch.cat((y_test_t,  labels), 0)\n",
    "            y_pred_t = torch.cat((y_pred_t, ps), 0)\n",
    "            \n",
    "            test_acc.append(np.mean(multi_label_accuracy(y_test_t, y_pred_t)))\n",
    "            test_f1.append(np.mean(multi_label_f1(y_test_t, y_pred_t)))\n",
    "            test_auroc.append(np.mean(multi_label_auroc(y_test_t, y_pred_t)))\n",
    "\n",
    "            p,r = multi_label_precision_recall(y_test_t, y_pred_t)\n",
    "            test_precision.append(np.mean(p))\n",
    "            test_recall.append(np.mean(r))\n",
    "\n",
    "        test_auroc = np.nanmean(test_auroc)\n",
    "        test_acc = np.nanmean(test_acc)\n",
    "        test_f1 = np.nanmean(test_f1)\n",
    "        test_precision = np.nanmean(test_precision)\n",
    "        test_recall = np.nanmean(test_recall)\n",
    "\n",
    "\n",
    "        print(\"AUROC : \",test_auroc)\n",
    "        print(\"Accuracy : \",test_acc)\n",
    "        print(\"f1 score : \",test_f1)\n",
    "        print(\"precision score : \",test_precision)\n",
    "        print(\"recall score : \",test_recall)\n",
    "\n",
    "        eval_matrix = [modeltxt,test_auroc,test_acc,test_f1,test_precision,test_recall]\n",
    "\n",
    "        return y_test_t,y_pred_t,eval_matrix\n",
    "\n",
    "# Plot\n",
    "def plot_conf(y_test,y_pred,modeltxt):\n",
    "    f, axes = plt.subplots(2, 4, figsize=(14, 8))\n",
    "    f.suptitle('Confustion Matrix For Model '+ modeltxt, fontsize=20, fontweight='bold')\n",
    "    plt.rcParams.update({'font.size': 12,'font.weight': 'bold'})\n",
    "    y_test = y_test.cpu()\n",
    "    y_pred = np.where(y_pred.cpu()>0.5,1,0)\n",
    "    axes = axes.ravel()\n",
    "    for i in range(8):\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix(y_test[:, i],\n",
    "                                                       y_pred[:, i]))\n",
    "        disp.plot(ax=axes[i], values_format='.10g')\n",
    "        disp.ax_.set_title(f'{LABELS[i]}')\n",
    "        disp.im_.colorbar.remove()\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.25, hspace=0.25)\n",
    "    plt.show()\n",
    "\n",
    "# Get Values\n",
    "def get_conf(y_test,y_pred,modeltxt):\n",
    "    conf_vals = []\n",
    "    y_test = y_test.cpu()\n",
    "    y_pred = np.where(y_pred.cpu()>0.5,1,0)\n",
    "    for i in range(8):\n",
    "        c = confusion_matrix(y_test[:, i],y_pred[:, i])\n",
    "        try:\n",
    "            p = c[0][0]\n",
    "        except IndexError:\n",
    "            p = 0\n",
    "        try:\n",
    "            q = c[0][1]\n",
    "        except IndexError:\n",
    "            q = 0\n",
    "        try:\n",
    "            r = c[1][0]\n",
    "        except IndexError:\n",
    "            r = 0\n",
    "        try:\n",
    "            s = c[1][1]\n",
    "        except IndexError:\n",
    "            s = 0\n",
    "\n",
    "        conf_vals.append([modeltxt,LABELS[i],p,q,r,s])\n",
    "\n",
    "    return conf_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc11c6",
   "metadata": {},
   "source": [
    "### Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_matrix_all = []\n",
    "for mName in mName_list:\n",
    "    print(\"Predicting for Model \",mName)\n",
    "    globals()[f\"y_test_t_{mName}\"],globals()[f\"y_pred_t_{mName}\"],eval_matrix = getTestPreds(globals()[f\"best_model_{mName}\"],test_dataloader,mName)\n",
    "    eval_matrix_all.append(eval_matrix)\n",
    "    print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e27c9d",
   "metadata": {},
   "source": [
    "### Print and Plot Evaluation Matrices\n",
    "Print AUROC, Accuracy, F1 Score, Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe11a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mat_all = pd.DataFrame(eval_matrix_all)\n",
    "df_mat_all.columns = [\"Model Name\",\"AUROC\",\"Accuracy\",\"F1 Score\",\"Precision\",\"Recall\"]\n",
    "df_mat_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06db9de",
   "metadata": {},
   "source": [
    "### Print Accuracy for Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7283352",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list_all = []\n",
    "for mName in mName_list:\n",
    "    for i in range(8):\n",
    "        acc = accuracy_score(globals()[f\"y_test_t_{mName}\"].to(\"cpu\").numpy()[:, i], np.where(globals()[f\"y_pred_t_{mName}\"].to(\"cpu\").numpy()[:, i]>=0.5,1,0))\n",
    "        p = precision_recall_fscore_support(globals()[f\"y_test_t_{mName}\"].to(\"cpu\").numpy()[:, i], np.where(globals()[f\"y_pred_t_{mName}\"].to(\"cpu\").numpy()[:, i]>=0.5,1,0),average='binary')\n",
    "        #print(LABELS[i],\" ==> \",\"Acc : \",acc,\" Precision : \",p[0],\" Recall : \",p[1])\n",
    "        if i !=12:\n",
    "            auroc = roc_auc_score(globals()[f\"y_test_t_{mName}\"].to(\"cpu\").numpy()[:, i], globals()[f\"y_pred_t_{mName}\"].to(\"cpu\").numpy()[:, i])\n",
    "        else:\n",
    "            auroc = np.nan\n",
    "\n",
    "        #print(LABELS[i],p)\n",
    "        label_list_all.append([mName,LABELS[i],auroc,acc,p[0],p[1]])\n",
    "        \n",
    "df_label_list_all = pd.DataFrame(label_list_all)\n",
    "df_label_list_all.columns = [\"Model Name\",\"Label\",\"AUROC\",\"Accuracy\",\"Precision\",\"Recall\"]\n",
    "df_label_list_all.to_csv(out+\"df_label_list_all.csv\",index=False)\n",
    "df_label_list_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e35da8",
   "metadata": {},
   "source": [
    "### AUROC of Labels accross Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f3c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_list_all.pivot(index='Label', columns='Model Name', values=[\"AUROC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3200e",
   "metadata": {},
   "source": [
    "### Accuracy of Labels accross Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed65b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_list_all.pivot(index='Label', columns='Model Name', values=[\"Accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57313d31",
   "metadata": {},
   "source": [
    "### Precision of Labels accross Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_list_all.pivot(index='Label', columns='Model Name', values=[\"Precision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_list_all.pivot(index='Label', columns='Model Name', values=[\"Recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599be3f",
   "metadata": {},
   "source": [
    "### Print Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_all = pd.DataFrame()\n",
    "for mName in mName_list:\n",
    "    print(\"Print Confusion Matrix for Model \",mName)\n",
    "    conf_all = conf_all.append(pd.DataFrame(get_conf(globals()[f\"y_test_t_{mName}\"],globals()[f\"y_pred_t_{mName}\"],mName)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce57810",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_all.columns = [\"Model Name\",\"Label\",\"True Negative\",\"False Positive\",\"False Negative\",\"True Positive\"]\n",
    "conf_all.to_csv(\"ConfusionMatrix.csv\",index=False)\n",
    "conf_all_piv = conf_all.pivot(index='Label', columns='Model Name', values=[\"True Negative\",\"False Positive\",\"False Negative\",\"True Positive\"])\n",
    "conf_all_piv.to_csv(out+\"ConfusionMatrixPivot.csv\",index=False)\n",
    "conf_all_piv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde93cd",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_all = pd.DataFrame()\n",
    "for mName in mName_list:\n",
    "    print(\"Plot Confusion Matrix for Model \",mName)\n",
    "    plot_conf(globals()[f\"y_test_t_{mName}\"],globals()[f\"y_pred_t_{mName}\"],mName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61313098",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Training Loss Curve CustumNet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_matrix_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db577ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mName in mName_list:\n",
    "    print(\"Predicting for Model\", mName)\n",
    "    y_test_t = globals()[f\"y_test_t_{mName}\"]\n",
    "    y_pred_t = globals()[f\"y_pred_t_{mName}\"]\n",
    "\n",
    "    # Código para trazar curvas ROC\n",
    "    auc_roc_vals = []\n",
    "    for i in range(len(LABELS)):\n",
    "        try:\n",
    "            gt = np.array(y_test_t.cpu()[:, i])\n",
    "            pred = y_pred_t.cpu()[:, i]\n",
    "            gt = gt.astype('int64')\n",
    "            gt = gt.reshape(-1, 1)\n",
    "            auc_roc = roc_auc_score(gt, pred)\n",
    "            print(auc_roc)\n",
    "            auc_roc_vals.append(auc_roc)\n",
    "            fpr_rf, tpr_rf, _ = roc_curve(gt, pred)\n",
    "            plt.figure(1, figsize=(10, 10))\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(fpr_rf, tpr_rf,\n",
    "                     label=LABELS[i] + \" (\" + str(round(auc_roc, 3)) + \")\")\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title(f'ROC curve for {mName}')\n",
    "            plt.legend(loc='best')\n",
    "        except ValueError:\n",
    "            pass\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed08338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_matrix_all = []\n",
    "# for mName in mName_list:\n",
    "#     print(\"Predicting for Model \")\n",
    "#     y_test_t, y_pred_t, eval_matrix = getTestPreds(globals()[f\"best_model_{mName}\"], test_dataloader, mName)\n",
    "#     eval_matrix_all.append(eval_matrix)\n",
    "\n",
    "#     # Código para trazar curvas ROC\n",
    "#     auc_roc_vals = []\n",
    "#     for i in range(len(LABELS)):\n",
    "#         try:\n",
    "#             gt = np.array(y_test_t.cpu()[:, i])\n",
    "#             pred = y_pred_t.cpu()[:, i]\n",
    "#             gt = gt.astype('int64')\n",
    "#             gt = gt.reshape(-1, 1)\n",
    "#             auc_roc = roc_auc_score(gt, pred)\n",
    "#             print(auc_roc)\n",
    "#             auc_roc_vals.append(auc_roc)\n",
    "#             fpr_rf, tpr_rf, _ = roc_curve(gt, pred)\n",
    "#             plt.figure(1, figsize=(10, 10))\n",
    "#             plt.plot([0, 1], [0, 1], 'k--')\n",
    "#             plt.plot(fpr_rf, tpr_rf,\n",
    "#                      label=LABELS[i] + \" (\" + str(round(auc_roc, 3)) + \")\")\n",
    "#             plt.xlabel('False positive rate')\n",
    "#             plt.ylabel('True positive rate')\n",
    "#             plt.title('ROC curve')\n",
    "#             plt.legend(loc='best')\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f1d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac42b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab2562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Asegúrate de que todas las listas tengan la misma longitud\n",
    "# min_length = min(len(globals()[f\"best_score_acc_CustomNet\"]),\n",
    "#                  len(globals()[f\"best_score_f1_CustomNet\"]))\n",
    "\n",
    "# # Utiliza solo las primeras `min_length` épocas para evitar la discrepancia de dimensiones\n",
    "# x = x[:min_length]\n",
    "\n",
    "# # Crear subgráficos\n",
    "# fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12, 10))\n",
    "\n",
    "# # Gráfico de precisión de entrenamiento y validación\n",
    "# ax[0].set_title('AUROC vs. Epochs')\n",
    "# ax[0].plot(x, globals()[f\"best_score_acc_CustomNet\"][:min_length], 'o-', label='AUROC')\n",
    "# ax[0].set_xlabel('Epochs')\n",
    "# ax[0].set_ylabel('AUROC')\n",
    "# ax[0].legend(loc='best')\n",
    "\n",
    "# # Gráfico de precisión de entrenamiento y validación\n",
    "# ax[1].set_title('Accuracy vs. Epochs')\n",
    "# ax[1].plot(x, globals()[f\"best_score_acc_CustomNet\"][:min_length], 'o-', label='Accuracy')\n",
    "# ax[1].set_xlabel('Epochs')\n",
    "# ax[1].set_ylabel('Accuracy')\n",
    "# ax[1].legend(loc='best')\n",
    "\n",
    "# # Gráfico de pérdida de entrenamiento y validación\n",
    "# ax[2].set_title('F1 Score vs. Epochs')\n",
    "# ax[2].plot(x, globals()[f\"best_score_f1_CustomNet\"][:min_length], 'o-', label='F1 Score')\n",
    "# ax[2].set_xlabel('Epochs')\n",
    "# ax[2].set_ylabel('F1 Score')\n",
    "# ax[2].legend(loc='best')\n",
    "\n",
    "# # Ajustar el diseño y mostrar el gráfico\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63261645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista de épocas\n",
    "x = list(range(MAX_EPOCHS))\n",
    "\n",
    "# Crear subgráficos\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12, 10))\n",
    "\n",
    "# Gráfico de precisión de entrenamiento y validación\n",
    "ax[0].set_title('AUROC vs. Epochs')\n",
    "ax[0].plot(x, globals()[f\"best_score_acc_CustomNet\"], 'o-', label='AUROC')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('AUROC')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "# Gráfico de precisión de entrenamiento y validación\n",
    "ax[1].set_title('Accuracy vs. Epochs')\n",
    "ax[1].plot(x, globals()[f\"best_score_acc_CustomNet\"], 'o-', label='Accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "# Gráfico de pérdida de entrenamiento y validación\n",
    "ax[2].set_title('F1 Score vs. Epochs')\n",
    "ax[2].plot(x, globals()[f\"best_score_f1_CustomNet\"], 'o-', label='F1 Score')\n",
    "ax[2].set_xlabel('Epochs')\n",
    "ax[2].set_ylabel('F1 Score')\n",
    "ax[2].legend(loc='best')\n",
    "\n",
    "# Ajustar el diseño y mostrar el gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf641159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
